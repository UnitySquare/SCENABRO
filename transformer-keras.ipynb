{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rEDsnqvT-AqQq4kV8ejILdKpl080uE0L","authorship_tag":"ABX9TyMsLn3sdbshJHIsZDSFvabd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install konlpy\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from konlpy.tag import Okt\n","from sklearn.model_selection import train_test_split\n","\n","# 태그 단어\n","PAD = \"<PADDING>\"   # 패딩\n","STA = \"<START>\"     # 시작\n","END = \"<END>\"       # 끝\n","OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n","\n","# 태그 인덱스\n","PAD_INDEX = 0\n","STA_INDEX = 1\n","END_INDEX = 2\n","OOV_INDEX = 3\n","\n","# 데이터 타입\n","ENCODER_INPUT  = 0\n","DECODER_INPUT  = 1\n","DECODER_TARGET = 2\n","\n","# 한 문장에서 단어 시퀀스의 최대 개수\n","max_sequences = 30\n","\n","# 임베딩 벡터 차원\n","embedding_dim = 100\n","\n","# LSTM 히든레이어 차원\n","lstm_hidden_dim = 512\n","\n","# 정규 표현식 필터\n","RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n","\n","# Data Frame Create\n","story_df = pd.read_csv('/content/drive/MyDrive/my_ws/project/Final/Data/story.csv', encoding='cp949')\n","\n","# 1. 정규식을 사용해서 특수문자, 숫자 제거\n","# story_df의 각 열에 대해서 정규 표현식을 사용하여 문자열 치환을 수행하는 반복문입니다.\n","for i in range(10):  # 0부터 9까지의 열 인덱스\n","    story_df[str(i)] = story_df[str(i)].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]', '')\n","\n","story_df.drop(['결론'], axis=1, inplace=True)\n","\n","# 결과를 저장할 빈 데이터프레임 리스트를 초기화합니다.\n","dataframes = []\n","\n","# story_df에서 9열까지 있다고 가정합니다 (0부터 9까지, 총 10개의 열).\n","# 0열부터 8열까지를 input으로 하고, 그 다음 열을 output으로 하는 데이터프레임을 생성합니다.\n","for i in range(9):  # 0부터 8까지 (9는 포함되지 않음)\n","    # 각 행에 대해 원하는 열을 공백으로 구분하여 합치기 위한 람다 함수를 apply 메서드에 사용합니다.\n","    input_column = story_df.iloc[:, :(i + 1)].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","    output_column = story_df.iloc[:, i + 1]\n","\n","    # 새로운 데이터프레임을 만들고 dataframes 리스트에 추가합니다.\n","    df = pd.DataFrame({'input': input_column, 'output': output_column})\n","    dataframes.append(df)\n","\n","# 모든 데이터프레임을 세로 방향으로 합쳐서 result_df를 생성합니다.\n","result_df = pd.concat(dataframes, ignore_index=True)\n","\n","# 결과를 출력합니다.\n","result_df\n","\n","x_data_df = result_df['input'];\n","t_data_df = result_df['output'];\n","\n","x_data_train, x_data_test, t_data_train, t_data_test = \\\n","train_test_split(x_data_df,\n","                 t_data_df,\n","                 test_size=0.2,\n","                 random_state=42)\n","\n","question, answer = list(x_data_train), list(t_data_train)\n","# 챗봇 데이터 출력\n","for i in range(5):\n","    print('Q : ' + question[i])\n","    print('A : ' + answer[i])\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMi8BzFnRAcl","executionInfo":{"status":"ok","timestamp":1704687212984,"user_tz":-540,"elapsed":6131,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"c012ec8e-18f9-4c25-fb63-a7fb27ec5d4d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n","Q : 알렉스는 특별한 능력을 가진 청년으로 그의 능력은 남들이 볼 수 없는 꿈의 세계를 볼 수 있다는 것입니다\n","A : 그는 이 능력을 숨기며 평범한 삶을 살고 있었지만 어느 날 그의 꿈에 미지의 여성 에밀리가 나타납니다\n","\n","Q : 영화는 평범한 삶을 살던 주인공 제이크의 이야기로 시작된다 제이크는 어느 날 잠에서 깨어나면 다른 세상에 떨어진 것을 발견한다 그곳은 고대 시대로 제이크는 거대한 동물들 사이에서 생존을 위한 투쟁을 시작한다 그러나 그는 점차 그 세상의 독특한 문화와 사람들에게 매료되어 간다\n","A : 그리고 그는 고대 세상의 여왕 레나와 사랑에 빠진다\n","\n","Q : 이야기는 외계 행성에 살고 있는 과학자 훈이의 일상에서 시작된다 훈은 외계 생명체 연구를 통해 새로운 에너지원을 발견하게 된다 그러나 그의 발견은 비밀스럽게 행성을 지배하는 조직의 이목을 끈다\n","A : 조직은 훈의 연구를 이용해 행성을 지배하려는 계획을 세운다\n","\n","Q : 조용한 작은 마을에서 살던 청년 톰은 어느 날 자신이 마법의 힘을 가진 사람임을 알게 된다 이는 그의 할아버지로부터 전해진 비밀로 그는 금방 이 힘을 제어하는 법을 배우지 못한다 그의 마법은 자주 부주의하게 행동하며 톰을 곤경에 빠뜨린다 그러나 그는 이 힘을 바른 방향으로 사용하기 위해 노력하며 자신만의 능력을 발견한다 그는 주변 사람들의 도움을 받아 마법의 힘을 제어하는 법을 배우고 그 힘을 사용해 사람들을 도와주기 시작한다 그의 마법은 마을 사람들에게 기쁨과 행복을 주며 그는 마을의 영웅이 된다 그러나 어둠의 마법사가 그의 마법의 힘을 원하며 마을을 위협하기 시작한다\n","A : 톰은 마을을 지키기 위해 어둠의 마법사와 대결하며 그의 진정한 용기를 보여준다\n","\n","Q : 서울의 한 가운데 잘 알려지지 않은 작은 카페에서 주인공 김수현은 평범한 일상을 살아가고 있었습니다 그러던 어느 날 그의 카페에 미스터리한 여성 유나가 찾아오면서 이야기는 새로운 방향으로 흘러가게 됩니다 유나는 과거의 아픈 기억을 갖고 있었고 그로 인해 현재에 갇혀 살고 있었습니다 김수현은 그녀의 아픔을 공감하며 함께 과거를 극복하려 노력하게 됩니다\n","A : 그들은 함께 시간을 보내며 서로에게 의지하게 되고 서로의 아픈 과거를 치유해 나가는 격렬한 시간을 보냅니다\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-10-482cdc428ab9>:46: FutureWarning: The default value of regex will change from True to False in a future version.\n","  story_df[str(i)] = story_df[str(i)].str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ ]', '')\n"]}]},{"cell_type":"code","source":["# 형태소분석 함수\n","def pos_tag(sentences):\n","\n","    # KoNLPy 형태소분석기 설정\n","    tagger = Okt()\n","\n","    # 문장 품사 변수 초기화\n","    sentences_pos = []\n","\n","    # 모든 문장 반복\n","    for sentence in sentences:\n","        # 특수기호 제거\n","        sentence = re.sub(RE_FILTER, \"\", sentence)\n","\n","        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n","        sentence = \" \".join(tagger.morphs(sentence))\n","        sentences_pos.append(sentence)\n","\n","    return sentences_pos\n","\n","# 형태소분석 수행\n","question = pos_tag(question)\n","answer = pos_tag(answer)\n","\n","# 형태소분석으로 변환된 챗봇 데이터 출력\n","for i in range(10):\n","    print('Q : ' + question[i])\n","    print('A : ' + answer[i])\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taLeEbPbRiBx","executionInfo":{"status":"ok","timestamp":1704687238825,"user_tz":-540,"elapsed":23883,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"a2c128c1-e235-45e2-897d-4a059bd86d9c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Q : 알렉스 는 특별한 능력 을 가진 청년 으로 그 의 능력 은 남 들 이 볼 수 없는 꿈 의 세계 를 볼 수 있다는 것 입니다\n","A : 그 는 이 능력 을 숨기 며 평범한 삶 을 살 고 있었지만 어느 날 그 의 꿈 에 미지 의 여성 에밀리 가 나타납니다\n","\n","Q : 영화 는 평범한 삶 을 살던 주인공 제이크 의 이야기 로 시작 된다 제이크 는 어느 날 잠 에서 깨어나면 다른 세상 에 떨어진 것 을 발견 한다 그 곳 은 고대 시대 로 제이크 는 거대한 동물 들 사이 에서 생존 을 위 한 투쟁 을 시작 한 다 그러나 그 는 점차 그 세상 의 독특한 문화 와 사람 들 에게 매료 되어 간다\n","A : 그리고 그 는 고대 세상 의 여왕 레나 와 사랑 에 빠진다\n","\n","Q : 이야기 는 외계 행성 에 살 고 있는 과학자 훈이 의 일상 에서 시작 된다 훈 은 외계 생명체 연구 를 통해 새로운 에너지 원 을 발견 하게 된다 그러나 그 의 발견 은 비밀 스럽게 행성 을 지배 하는 조직 의 이목 을 끈다\n","A : 조직 은 훈 의 연구 를 이용 해 행성 을 지배 하려는 계획 을 세운다\n","\n","Q : 조용한 작은 마을 에서 살던 청년 톰 은 어느 날 자신 이 마법 의 힘 을 가진 사람 임 을 알 게 된다 이는 그 의 할아버지 로부터 전해진 비밀 로 그 는 금방 이 힘 을 제어 하는 법 을 배우지 못 한다 그 의 마법 은 자주 부주의하게 행동 하며 톰 을 곤경 에 빠뜨린다 그러나 그 는 이 힘 을 바른 방향 으로 사용 하기 위해 노력 하며 자신 만의 능력 을 발견 한다 그 는 주변 사람 들 의 도움 을 받아 마법 의 힘 을 제어 하는 법 을 배우고 그 힘 을 사용 해 사람 들 을 도와주기 시작 한 다 그 의 마법 은 마을 사람 들 에게 기쁨 과 행복 을 주며 그 는 마을 의 영웅 이 된다 그러나 어둠 의 마법사 가 그 의 마법 의 힘 을 원하며 마을 을 위협 하기 시작 한 다\n","A : 톰 은 마을 을 지키기 위해 어둠 의 마법사 와 대결 하며 그 의 진정한 용기 를 보여준다\n","\n","Q : 서울 의 한 가운데 잘 알려지지 않은 작은 카페 에서 주인공 김수현 은 평범한 일상 을 살아가고 있었습니다 그러던 어느 날 그 의 카페 에 미스터리 한 여성 유나 가 찾아오면서 이야기 는 새로운 방향 으로 흘러가게 됩니다 유나 는 과거 의 아픈 기억 을 갖고 있었고 그로 인해 현재 에 갇혀 살 고 있었습니다 김수현 은 그녀 의 아픔 을 공감 하며 함께 과거 를 극복 하려 노력 하게 됩니다\n","A : 그 들 은 함께 시간 을 보내며 서로 에게 의지 하게 되고 서로 의 아픈 과거 를 치유 해 나가는 격렬한 시간 을 보냅니다\n","\n","Q : 마지막 전사 라는 별명 을 가진 레오 는 세상 에서 가장 강력한 무기 를 통제 하는 고대 의 씨앗 을 찾아야 하는 중요한 임무 를 맡게 된다 레오 는 이 임무 를 완수 하기 위해 세계 각지 를 여행 하며 그 과정 에서 다양한 문화 와 인물 들 을 만나게 된다 그러나 그 의 임무 는 예상 보다 어렵게 느껴지고 씨앗 을 찾는 것 이 그 에게 주어진 능력 을 시험 하는 일임 을 깨닫게 된다 그러던 중 레오 는 씨앗 의 위치 를 알 고 있는 신비한 여성 아리아 를 만나게 된다 아리아 는 레오 에게 씨앗 의 진정한 가치 와 그것 을 통제 하는 것 이 가져올 수 있는 위험성 을 설명 한다 아리아 의 도움 으로 레오 는 고대 의 씨앗 을 찾아내지만 그것 을 통제 하려는 악의 세력 이 나타나 전투 가 시작 된다\n","A : 레오 는 자신 의 능력 과 용기 를 믿고 세상 을 구 하기 위해 싸우기로 결정 한다\n","\n","Q : 해적선 의 요리사 로 일 하던 성준 은 폭풍우 속 에서 신비로운 보물지도 를 발견 한다 그 는 선장 의 탐욕 을 피해 몰래 지도 를 갖고 탈출 을 감행 한다\n","A : 육지 에 도착 한 성준 은 지도 가 가리키는 전설 속 의 섬 을 찾기 로 결심 한다\n","\n","Q : 서울 에 살 고 있는 사진작가 지현 은 어느 날 자신 의 사진 속 에서 실종 된 여자 아이 의 모습 을 발견 한다 사진 속 의 아이 는 매 일 매일 다른 장소 에 나타나며 지현 은 이 현상 을 이해 할 수 없게 된다 지현 은 이 현상 이 무엇 인지 알아내기 위해 사진 속 의 아이 가 나타나는 장소 를 직접 찾아가게 된다\n","A : 그 곳 에서 지현 은 아이 의 실종 사건 과 관련 된 실마리 를 찾게 되고 점점 아이 를 찾는 데 에 집착 하게 된다\n","\n","Q : 태호 는 친구 들 과 함께 도심 속 폐허 가 된 공장 에서 탐험 을 하는 것 을 좋아하는 대학생 이다 어느 날 그 들 은 공장 깊은 곳 에서 낡은 로봇 A 를 발견 하고 우연히 그것 을 작동시킨다 A 는 과거 대기업 유 트론 이 개발 한 실험 용 로봇 으로 인간 의 감정 을 이해 할 수 있다고 한다 태호 는 로봇 과의 교감 을 통해 A 에게 인간 세상 을 가르치기 시작 한 다\n","A : 하지만 유 트론 은 로봇 의 존재 를 감지 하고 A 를 회수 하기 위해 추적 을 시작 한 다\n","\n","Q : 한때 유명한 아동 소설가 였던 소희 는 창작 의 고민 에 빠져 글 을 쓰지 못 하는 나날 을 보냅니다\n","A : 어느 날 그녀 의 소설 속 주인공 인 호기심 많은 소년 루카 가 현실 세계 로 뛰 쳐나오면서 이야기 는 시작 됩니다\n","\n"]}]},{"cell_type":"code","source":["# 질문과 대답 문장들을 하나로 합침\n","sentences = []\n","sentences.extend(question)\n","sentences.extend(answer)\n","\n","words = []\n","\n","# 단어들의 배열 생성\n","for sentence in sentences:\n","    for word in sentence.split():\n","        words.append(word)\n","\n","# 길이가 0인 단어는 삭제\n","words = [word for word in words if len(word) > 0]\n","\n","# 중복된 단어 삭제\n","words = list(set(words))\n","\n","# 제일 앞에 태그 단어 삽입\n","words[:0] = [PAD, STA, END, OOV]\n","\n","# 단어와 인덱스의 딕셔너리 생성\n","word_to_index = {word: index for index, word in enumerate(words)}\n","index_to_word = {index: word for index, word in enumerate(words)}"],"metadata":{"id":"QrksdQqeR1B-","executionInfo":{"status":"ok","timestamp":1704687282902,"user_tz":-540,"elapsed":236,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 문장을 인덱스로 변환\n","def convert_text_to_index(sentences, vocabulary, type):\n","\n","    sentences_index = []\n","\n","    # 모든 문장에 대해서 반복\n","    for sentence in sentences:\n","        sentence_index = []\n","\n","        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n","        if type == DECODER_INPUT:\n","            sentence_index.extend([vocabulary[STA]])\n","\n","        # 문장의 단어들을 띄어쓰기로 분리\n","        for word in sentence.split():\n","            if vocabulary.get(word) is not None:\n","                # 사전에 있는 단어면 해당 인덱스를 추가\n","                sentence_index.extend([vocabulary[word]])\n","            else:\n","                # 사전에 없는 단어면 OOV 인덱스를 추가\n","                sentence_index.extend([vocabulary[OOV]])\n","\n","        # 최대 길이 검사\n","        if type == DECODER_TARGET:\n","            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n","            if len(sentence_index) >= max_sequences:\n","                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n","            else:\n","                sentence_index += [vocabulary[END]]\n","        else:\n","            if len(sentence_index) > max_sequences:\n","                sentence_index = sentence_index[:max_sequences]\n","\n","        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n","        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n","\n","        # 문장의 인덱스 배열을 추가\n","        sentences_index.append(sentence_index)\n","\n","    return np.asarray(sentences_index)"],"metadata":{"id":"4451MTKJR63a","executionInfo":{"status":"ok","timestamp":1704687285527,"user_tz":-540,"elapsed":264,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 인덱스를 문장으로 변환\n","def convert_index_to_text(indexs, vocabulary):\n","\n","    sentence = ''\n","\n","    # 모든 문장에 대해서 반복\n","    for index in indexs:\n","        if index == END_INDEX:\n","            # 종료 인덱스면 중지\n","            break;\n","        elif vocabulary.get(index) is not None:\n","            # 사전에 있는 인덱스면 해당 단어를 추가\n","            sentence += vocabulary[index]\n","        else:\n","            # 사전에 없는 인덱스면 OOV 단어를 추가\n","            sentence += vocabulary[OOV_INDEX]\n","\n","        # 빈칸 추가\n","        sentence += ' '\n","\n","    return sentence"],"metadata":{"id":"O4DvgmGTnW8Z","executionInfo":{"status":"ok","timestamp":1704693004118,"user_tz":-540,"elapsed":295,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# 인코더 입력 인덱스 변환\n","x_encoder = convert_text_to_index(question, word_to_index, ENCODER_INPUT)\n","\n","# 첫 번째 인코더 입력 출력 (12시 땡)\n","print(x_encoder[0])\n","\n","# 디코더 입력 인덱스 변환\n","x_decoder = convert_text_to_index(answer, word_to_index, DECODER_INPUT)\n","\n","# 첫 번째 디코더 입력 출력 (START 하루 가 또 가네요)\n","print(x_decoder[0])\n","\n","# 디코더 목표 인덱스 변환\n","y_decoder = convert_text_to_index(answer, word_to_index, DECODER_TARGET)\n","\n","# 첫 번째 디코더 목표 출력 (하루 가 또 가네요 END)\n","print(y_decoder[0])\n","\n","# 원핫인코딩 초기화\n","one_hot_data = np.zeros((len(y_decoder), max_sequences, len(words)))\n","\n","# 디코더 목표를 원핫인코딩으로 변환\n","# 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n","for i, sequence in enumerate(y_decoder):\n","    for j, index in enumerate(sequence):\n","        one_hot_data[i, j, index] = 1\n","\n","# 디코더 목표 설정\n","y_decoder = one_hot_data\n","\n","# 첫 번째 디코더 목표 출력\n","print(y_decoder[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8IeD42rR9i8","executionInfo":{"status":"ok","timestamp":1704693549593,"user_tz":-540,"elapsed":280,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"1eb7b34b-c9e7-4bec-e355-dd24b1d410d5"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 664  229  260 1231 1279 1037 1110 2589 1345 2072 1231 2427 1152 2176\n","  849 2569  191  982  283 2072 2355 2157 2569  191 1208 1548  175    0\n","    0    0]\n","[   1 1345  229  849 1231 1279   10  565 1135 2204 1279  188 1937 1804\n","  752 2130 1345 2072  283 1648  575 2072 2508  791  859 1868    0    0\n","    0    0]\n","[1345  229  849 1231 1279   10  565 1135 2204 1279  188 1937 1804  752\n"," 2130 1345 2072  283 1648  575 2072 2508  791  859 1868    2    0    0\n","    0    0]\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WlZCUdRcE87m","executionInfo":{"status":"ok","timestamp":1704686832856,"user_tz":-540,"elapsed":3965,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim #입력 토큰 벡터의 크기\n","        self.dense_dim = dense_dim #내부 밀집 층의 크기\n","        self.num_heads = num_heads #어텐션 헤드의 개수\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None): #call()메서드에 연산을 수행한다.\n","        if mask is not None: #Embedding층에서 생성하는 마스크는 2D이지만 어텐션 층은 3D또는 4D를 기대한다.\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self): #모델 저장을 위한 직렬화 구현\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"]},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","    #위치 임베딩의 단점은 시퀀스 길이를 미리 알아야한다는 것\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding( #토큰 인덱스를 위한 Embedding층을 준비한다.\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions #두 임베딩 벡터를 더한다.\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"],"metadata":{"id":"vKwN2k9BIxC5","executionInfo":{"status":"ok","timestamp":1704686867562,"user_tz":-540,"elapsed":259,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True #이 속성은 층이 입력 마스킹을 출력으로 전달하도록 만든다.\n","        #케라스에서 마스킹을 사용하려면 명시적으로 설정을 해야한다.\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"],"metadata":{"id":"ZdAvpxciKLL2","executionInfo":{"status":"ok","timestamp":1704686871923,"user_tz":-540,"elapsed":1,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["vocab_size = 20000\n","sequence_length = 30\n","embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","#소스 문장을 인코딩한다.\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","#타깃 시퀀스를 인코딩하고 인코딩된 소스 문장과 합친다.\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) #출력 위치마다 하나의 단어를 예측한다\n","\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"metadata":{"id":"LpEiWjnzLOCh","executionInfo":{"status":"ok","timestamp":1704693561879,"user_tz":-540,"elapsed":1333,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","y_decoder_reshaped = np.reshape(y_decoder, (-1, sequence_length))\n","\n","history = transformer.fit(\n","    [x_encoder, x_decoder],\n","    y_decoder_reshaped,\n","    epochs=10,\n","    validation_split=0.2\n",")"],"metadata":{"id":"p7sviWSdLTqk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704693881786,"user_tz":-540,"elapsed":318828,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"40ba181a-7240-482b-c16d-ac7ccf4c6182"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","18/18 [==============================] - 36s 2s/step - loss: 3.8869 - accuracy: 0.9433 - val_loss: 0.8147 - val_accuracy: 1.0000\n","Epoch 2/10\n","18/18 [==============================] - 32s 2s/step - loss: 0.3533 - accuracy: 0.9995 - val_loss: 0.0809 - val_accuracy: 1.0000\n","Epoch 3/10\n","18/18 [==============================] - 33s 2s/step - loss: 0.0650 - accuracy: 0.9995 - val_loss: 0.0205 - val_accuracy: 1.0000\n","Epoch 4/10\n","18/18 [==============================] - 30s 2s/step - loss: 0.0238 - accuracy: 0.9995 - val_loss: 0.0066 - val_accuracy: 1.0000\n","Epoch 5/10\n","18/18 [==============================] - 32s 2s/step - loss: 0.0114 - accuracy: 0.9995 - val_loss: 0.0024 - val_accuracy: 1.0000\n","Epoch 6/10\n","18/18 [==============================] - 30s 2s/step - loss: 0.0072 - accuracy: 0.9995 - val_loss: 0.0011 - val_accuracy: 1.0000\n","Epoch 7/10\n","18/18 [==============================] - 30s 2s/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 5.0928e-04 - val_accuracy: 1.0000\n","Epoch 8/10\n","18/18 [==============================] - 33s 2s/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 3.7221e-04 - val_accuracy: 1.0000\n","Epoch 9/10\n","18/18 [==============================] - 30s 2s/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 3.3478e-04 - val_accuracy: 1.0000\n","Epoch 10/10\n","18/18 [==============================] - 32s 2s/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 4.0984e-04 - val_accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["max_decoded_sentence_length = 30\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = input_sentence\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = convert_text_to_index(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = convert_index_to_text(sampled_token_index, index_to_word)\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence"],"metadata":{"id":"A3z7JlpGSyQM","executionInfo":{"status":"ok","timestamp":1704692432257,"user_tz":-540,"elapsed":277,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# 텍스트 생성\n","def generate_text(input_seq):\n","\n","    # 입력을 인코더에 넣어 마지막 상태 구함\n","    states = encoder_model.predict(input_seq)\n","\n","    # 목표 시퀀스 초기화\n","    target_seq = np.zeros((1, 1))\n","\n","    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n","    target_seq[0, 0] = STA_INDEX\n","\n","    # 인덱스 초기화\n","    indexs = []\n","\n","    # 디코더 타임 스텝 반복\n","    while 1:\n","        # 디코더로 현재 타임 스텝 출력 구함\n","        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n","        decoder_outputs, \\\n","        state_text_forward_h, state_text_forward_c, \\\n","        state_text_backward_h, state_text_backward_c = decoder_model.predict(\n","                                                [target_seq] + states)\n","\n","        # 결과의 원핫인코딩 형식을 인덱스로 변환\n","        index = np.argmax(decoder_outputs[0, 0, :])\n","        indexs.append(index)\n","\n","        # 종료 검사\n","        if len(indexs) >= max_sequences:\n","            break\n","\n","        # 목표 시퀀스를 바로 이전의 출력으로 설정\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = index\n","\n","        # 디코더의 이전 상태를 다음 디코더 예측에 사용\n","        states = [state_text_forward_h, state_text_forward_c, state_text_backward_h, state_text_backward_c]\n","\n","    # 인덱스를 문장으로 변환\n","    sentence = convert_index_to_text(indexs, index_to_word)\n","    return sentence"],"metadata":{"id":"FLCSFFobqBRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측을 위한 입력 생성\n","def make_predict_input(sentence):\n","\n","    sentences = []\n","    sentences.append(sentence)\n","    sentences = pos_tag(sentences)\n","    input_seq = convert_text_to_index(sentences, word_to_index, ENCODER_INPUT)\n","\n","    return input_seq"],"metadata":{"id":"njlJ1HSpYYUh","executionInfo":{"status":"ok","timestamp":1704688978781,"user_tz":-540,"elapsed":3,"user":{"displayName":"롤다","userId":"00587605280025492851"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 문장을 인덱스로 변환\n","input_seq = make_predict_input('이야기 거리가 없다')\n","input_seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4EYSy-NYZZ7","executionInfo":{"status":"ok","timestamp":1704692995075,"user_tz":-540,"elapsed":251,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"5e642d8a-8198-4834-ccd7-8597b152f3c2"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 178, 1708,  859,    3,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0]])"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["# 예측 모델로 텍스트 생성\n","predictions = decode_sequence(input_seq)\n","predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"id":"Y36BNKcLYcO8","executionInfo":{"status":"error","timestamp":1704693536832,"user_tz":-540,"elapsed":324,"user":{"displayName":"롤다","userId":"00587605280025492851"}},"outputId":"808bea26-caab-47b5-d0bb-290b6222db1c"},"execution_count":71,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"convert_text_to_index() missing 2 required positional arguments: 'vocabulary' and 'type'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-f070902e9ef7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 예측 모델로 텍스트 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-59-4d85e5c97a69>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[start]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_decoded_sentence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         tokenized_target_sentence = convert_text_to_index(\n\u001b[0m\u001b[1;32m      8\u001b[0m             [decoded_sentence])[:, :-1]\n\u001b[1;32m      9\u001b[0m         predictions = transformer(\n","\u001b[0;31mTypeError\u001b[0m: convert_text_to_index() missing 2 required positional arguments: 'vocabulary' and 'type'"]}]},{"cell_type":"code","source":["!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","!unzip -q spa-eng.zip\n","#파일 다운받기\n","\n","from sklearn.model_selection import train_test_split\n","\n","text_file = \"spa-eng/spa.txt\"\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    english, spanish = line.split(\"\\t\") #각 라인은 영어 구절과 이에 해당하는 스페인 번역을 포함해 탭으로 구분됨.\n","    spanish = \"[start] \" + spanish + \" [end]\" #형식 맞추기 위해 스타트와 엔드 추가\n","    text_pairs.append((english, spanish))\n","    #파일 파싱해보기\n","\n","import random\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples]"],"metadata":{"id":"KWpaYBNTSOjg"},"execution_count":null,"outputs":[]}]}